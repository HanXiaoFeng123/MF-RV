![image](https://github.com/user-attachments/assets/d218d22a-0095-4127-acf6-5c30005b6ab4)# MF-RV
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
![](https://img.shields.io/github/last-commit/Xiaofeng-Han-Res/MF-RV?color=green) 
[![](https://img.shields.io/badge/PRs-Welcome-%23FF4500)](https://github.com/Xiaofeng-Han-Res/MF-RV)
![](https://img.shields.io/github/stars/Xiaofeng-Han-Res/MF-RV?color=yellow)
![](https://img.shields.io/github/forks/Xiaofeng-Han-Res/MF-RV?color=lightblue) 

<div style="text-align: justify">

This is a compilation on "Multimodal Fusion for Robot Vision," including state-of-the-art benchmarks and datasets.  

</div>

## News

## Overview

## Table of Contents
- [MF-RV](#MF-RV)
  - [News](#news)
  - [Overview](#overview)
  - [Table of Contents](#table-of-contents)
  - [Related Surveys](#related-surveys)
  - [DataSets](#datasets)

## Related Surveys

## DataSets
* Objectfolder 2.0: A multisensory object dataset for sim2real transfer (CVPR, 2022) [[paper]](https://openaccess.thecvf.com/content/CVPR2022/papers/Gao_ObjectFolder_2.0_A_Multisensory_Object_Dataset_for_Sim2Real_Transfer_CVPR_2022_paper.pdf)
* Touch and Go: Learning from Human-Collected Vision and Touch Supplementary Material (NuerIPS, 2022) [[paper]](https://arxiv.org/pdf/2211.12498)
* Connecting Touch and Vision via Cross-Modal Prediction (CVPR, 2019) [[paper]](https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Connecting_Touch_and_Vision_via_Cross-Modal_Prediction_CVPR_2019_paper.pdf)

## Embodied Large Language Models
* Rt-1: Robotics transformer for real-world control at scale (arXiv, 2022) [[paper]](https://arxiv.org/pdf/2212.06817)
* Rt-2: Vision-language-action models transfer web knowledge to robotic control (arXiv, 2023) [[paper]](https://arxiv.org/pdf/2307.15818)
* Open x-embodiment: Robotic learning datasets and rt-x models (arXiv, 2023) [[paper]](https://arxiv.org/pdf/2310.08864)
* Rt-h: Action hierarchies using language (arXiv, 2024) [[paper]](https://arxiv.org/pdf/2403.01823)
* Autort: Embodied foundation models for large scale orchestration of robotic agents (arXiv, 2024) [[paper]](https://arxiv.org/pdf/2401.12963)
* Gr-2: A generative video-language-action model with web-scale knowledge for robot manipulation (arXiv, 2024) [[paper]](https://arxiv.org/pdf/2410.06158)
* Voxposer: Composable 3d value maps for robotic manipulation with language models (arXiv, 2023) [[paper]](https://arxiv.org/pdf/2307.05973)
* Rdt-1b: a diffusion foundation model for bimanual manipulation (arXiv, 2024) [[paper]](https://arxiv.org/pdf/2410.07864)
* Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation (arXiv, 2024) [[paper]](https://arxiv.org/pdf/2409.01652)
* Copa: General robotic manipulation through spatial constraints of parts with foundation models (arXiv, 2024) [[paper]](https://arxiv.org/pdf/2403.08248)
* OpenVLA: An Open-Source Vision-Language-Action Model (arXiv, 2024) [[paper]](https://arxiv.org/pdf/2406.09246)
* Palm-e: An embodied multimodal language model (arXiv, 2023) [[paper]](https://arxiv.org/pdf/2303.03378)
* Π0: A Vision-Language-Action Flow Model for General Robot Control (arXiv, 2024) [[paper]](https://arxiv.org/pdf/2410.24164?)
